{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSiXE1-KWMBJ"
      },
      "source": [
        "# **System Requirements Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK_TOxKuCjhv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/305909/charbert.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "861Gl0WOCmWq"
      },
      "outputs": [],
      "source": [
        "!bash charbert/requirements.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr8dy_8o1JWX"
      },
      "source": [
        "# **Output Path Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik_P5oNj1HjB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_5UgcdL6BMq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def direct(path):\n",
        "  os.makedirs(path, exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMBBUtcZ0szy"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/NLP/output'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awmp-XYYPdo4"
      },
      "source": [
        "# **Baseline Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKOc5QZFP1yT"
      },
      "source": [
        "- Fine-Tuning the pre-trained language model (bert-base-cased) on the English Wikipedia dataset via Masked Language Modeling (MLM) approach to enhance the model’s comprehension performance in English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oI5OLIIrQZ3z"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + '/wikipedia-en'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python3 /content/charbert/CharBERT/LM.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_data_file /content/charbert/data/wikipedia/en_train.txt \\\n",
        "    --eval_data_file  /content/charbert/data/wikipedia/en_validation.txt \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --term_vocab /content/charbert/data/dict/term_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --mlm_probability 0.10 \\\n",
        "    --input_nraws 1000 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 10000 \\\n",
        "    --block_size 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --mlm \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjlqjPlLRMjk"
      },
      "source": [
        "- Fine-Tuning the model on the SQuAD dataset via Question Answering approach to refine the model’s performance in English QA tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrOCOt7vSPP-"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + '/SQuAD'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python /content/charbert/CharBERT/SQuAD.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file /content/charbert/data/SQuAD/SQuAD_train.json \\\n",
        "    --predict_file /content/charbert/data/SQuAD/SQuAD_validation.json \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 2000 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x162BI18B4wu"
      },
      "source": [
        "# **Domain Adaptation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5vfIT1mUei2"
      },
      "source": [
        "- Fine-Tuning the pre-trained language model (bert-base-cased) on the PubMED dataset via Masked Language Modeling (MLM) approach to enhance the model’s comprehension performance in medical knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN9jrUjdUwT-"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + '/PubMED'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python3 /content/charbert/CharBERT/LM.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_data_file /content/charbert/data/PubMED/PubMED_train.txt \\\n",
        "    --eval_data_file  /content/charbert/data/PubMED/PubMED_validation.txt \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --term_vocab /content/charbert/data/dict/term_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --mlm_probability 0.10 \\\n",
        "    --input_nraws 1000 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 10000 \\\n",
        "    --block_size 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --mlm \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqMt98heUovm"
      },
      "source": [
        "- Fine-Tuning the model on the BioASQ dataset via Question Answering approach to refine the model’s performance in medical QA tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4u4XwuEvlfT"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + '/BioASQ'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python /content/charbert/CharBERT/SQuAD.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file /content/charbert/data/BioASQ/BioASQ_train.json \\\n",
        "    --predict_file /content/charbert/data/BioASQ/BioASQ_validation.json \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 2000 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHXNN3kDwV6c"
      },
      "source": [
        "# **Multilingual Context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAVwID6OV6D0"
      },
      "source": [
        "- Fine-Tuning the pre-trained multilingual language model (bert-base-multilingual-cased) on both English and German Wikipedia datasets via Masked Language Modeling (MLM) approach to enhance the model’s comprehension performance in English and German."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBB4a-xrwcml"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + '/wikipedia-en-de'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python3 /content/charbert/CharBERT/LM.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-multilingual-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_data_file /content/charbert/data/wikipedia/en_de_train.txt \\\n",
        "    --eval_data_file  /content/charbert/data/wikipedia/en_de_validation.txt \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --term_vocab /content/charbert/data/dict/term_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --mlm_probability 0.10 \\\n",
        "    --input_nraws 1000 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 10000 \\\n",
        "    --block_size 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --mlm \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDKr8vK8XC4Z"
      },
      "source": [
        "- Fine-Tuning the model on the MLQA dataset via Question Answering approach to refine the model’s performance in English and German QA tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x9OLnARxJ6W"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + '/MLQA'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python /content/charbert/CharBERT/SQuAD.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-multilingual-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file /content/charbert/data/MLQA/MLQA_train.json \\\n",
        "    --predict_file /content/charbert/data/MLQA/MLQA_validation.json \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 2000 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpmfplzCbiEV"
      },
      "source": [
        "#**Noise Resilience Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlvJiVyCbiEh"
      },
      "source": [
        "- Fine-Tuning the model on the adversarial version of the SQuAD dataset via Question Answering approach to refine the model’s performance in English QA tasks with morphological variations and typos in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB9Jsy-ibiEh"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + '/SQuAD-attack'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python /content/charbert/CharBERT/SQuAD.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file /content/charbert/data/SQuAD/char_SQuAD_train.json \\\n",
        "    --predict_file /content/charbert/data/SQuAD/char_SQuAD_validation.json \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 2000 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZi33rF7biEh"
      },
      "source": [
        "- Fine-Tuning the model on the adversarial version of the BioASQ dataset via Question Answering approach to refine the model’s performance in medical QA tasks with morphological variations and typos in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-rmy6odbiEh"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + 'BioASQ-attack'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python /content/charbert/CharBERT/SQuAD.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file /content/charbert/data/BioASQ/char_BioASQ_train.json \\\n",
        "    --predict_file /content/charbert/data/BioASQ/char_BioASQ_validation.json \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 2000 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir {OUTPUT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0VRDvgowyCk"
      },
      "source": [
        "- Fine-Tuning the model on the adversarial version of the MLQA dataset via Question Answering approach to refine the model’s performance in English and German QA tasks with morphological variations and typos in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6qbTdIHwzzw"
      },
      "outputs": [],
      "source": [
        "OUTPUT = PATH + 'MLQA-attack'\n",
        "direct(OUTPUT)\n",
        "\n",
        "!python /content/charbert/CharBERT/SQuAD.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-multilingual-cased \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --train_file /content/charbert/data/MLQA/char_MLQA_train.json \\\n",
        "    --predict_file /content/charbert/data/MLQA/char_MLQA_validation.json \\\n",
        "    --char_vocab /content/charbert/data/dict/bert_char_vocab \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 2 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --per_gpu_eval_batch_size 4 \\\n",
        "    --save_steps 2000 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir {OUTPUT}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
